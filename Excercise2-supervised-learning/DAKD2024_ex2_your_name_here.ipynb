{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rpio-nMAoDwq"
   },
   "source": [
    "<div class=\"alert alert-block\" style=\"color: green\">\n",
    "    <h1><center> DAKD 2024 EXERCISE 2: SUPERVISED LEARNING  </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in your name, student id number and email address\n",
    "#### name:\n",
    "#### student id:\n",
    "#### email:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90c9fF6woDwr"
   },
   "source": [
    "The previous exercise was about <i>data understanding</i> and <i>data preparation</i>, which formed the basis for the modeling phase of the data mining process. Many modeling techniques make assumptions about the data, so the exploration and preparation phases can't be ignored. Now, as we have checked the validity of data and familiarized ourselves with it, we can move on to the next stage of the Cross-Industry Standard Process for Data Mining (CRISP-DM), which is <font color = green>modeling</font>.\n",
    "\n",
    "The questions to be answered at this stage could include:\n",
    "\n",
    "- What kind of model architecture best fits our data?\n",
    "- How well does the model perform technically?\n",
    "- Could we improve its performance?\n",
    "- How do we evaluate the model's performance?\n",
    "\n",
    "<i>Machine learning</i> is a subfield of artificial intelligence that provides automatic, objective and data-driven techniques for modeling the data. Its two main branches are <i>supervised learning</i> and <i>unsupervised learning</i>, and in this exercise, we are going to use the former, <font color = green>supervised learning</font>, for classification and regression tasks.\n",
    "\n",
    "For classification, data remains the same as in the previous exercise, but I've already cleaned it up for you. Some data pre-processing steps are still required to ensure that it's in an appropriate format so that models can learn from it. Even though we are not conducting any major data exploration nor data preparation this time, <i>you should never forget it in your future data analyses</i>.\n",
    "\n",
    "-----\n",
    "\n",
    "#### General Guidance for Exercises\n",
    "\n",
    "- <b>Complete all tasks:</b> Make sure to answer all questions, even if you cannot get your script to fully work.\n",
    "- <b>Code clarity:</b> Write clear and readable code. Include comments to explain what your code does.\n",
    "- <b>Effective visualizations:</b> Ensure all plots have labeled axes, legends, and captions. Your visualizations should clearly represent the underlying data.\n",
    "- <b>Notebook organization:</b> You can add more code or markdown cells to improve the structure of your notebook as long as it maintains a logical flow.\n",
    "- <b>Submission:</b> Submit both the .ipynb and .html or .pdf versions of your notebook. Before finalizing your notebook, use the \"Restart & Run All\" feature to ensure it runs correctly.\n",
    "- <b>Grading criteria:</b>\n",
    "    - The grading scale is *Fail*/*Pass*/*Pass with honors* (+1).\n",
    "    - To pass, you must complete the required parts 1-4.\n",
    "    - To achieve Pass with honors, complete the bonus exercises.\n",
    "- <b>Technical issues:</b>\n",
    "    - If you encounter problems, start with an online search to find solutions but do not simply copy and paste code. Understand any code you use and integrate it appropriately.\n",
    "    - Cite all external sources used, whether for code or explanations.\n",
    "    - If problems persist, ask for help in the course discussion forum, at exercise sessions, or via email at tuhlei@utu.fi, aibekt@utu.fi.\n",
    "- <b>Use of AI and large language models:</b>\n",
    "    - We do not encourage the use of AI tools like ChatGPT. If you use them, critically evaluate their outputs.\n",
    "    - Describe how you used the AI tools in your work, including your input and how the output was beneficial.\n",
    "- <b>Time management:</b> Do not leave your work until the last moment. No feedback will be available during weekends.\n",
    "- <b>Additional notes:</b>\n",
    "    - You can find the specific deadlines and session times for each assignment on the Moodle course page.\n",
    "    - Ensure all your answers are conciseâ€”typically a few sentences per question.\n",
    "    - Your .ipynb notebook is expected to be run to completion, which means that it should execute without errors when all cells are run in sequence.\n",
    " are run in sequence.\n",
    "\n",
    "\n",
    "<font color = green> The guided exercise session is held on the 27th of November at 14:15-16:00, at lecture hall X, Natura building.</font>\n",
    "\n",
    "<font color = red size = 4>The deadline is the 2nd of December at 23:59</font>. Late submissions will not be accepted unless there is a valid excuse for an extension which should be asked **before** the original deadline.\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I2WLapM3BPc"
   },
   "source": [
    "### <font color = red> Packages needed for this exercise: </font>\n",
    "\n",
    "You can use other packages as well, but this excercise can be completed with those below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iypIAVquoDws"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization packages - matplotlib and seaborn\n",
    "# Remember that pandas is also handy and capable when it comes to plotting!\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning package - scikit-learn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Show the plots inline in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "## <font color = lightcoral>1. Classification using k-nearest neighbors </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CqWZYx2oDw3"
   },
   "source": [
    "We start exploring the world of data modeling by using the <font color = lightcoral>K-Nearest Neightbors (k-NN) algorithm</font>. The k-NN algorithm is a classic supervised machine learning technique based on the assumption that data points with similar features tend to belong to the same class, and thus are likely to be near each other in feature space.\n",
    "\n",
    "In our case, we'll use the k-NN algorithm to *predict the presence of cardiovascular disease* (CVD) using all the other variables as <font color = lightcoral>features</font> in the given data set. I.e. the <font color = lightcoral>target variable</font> that we are interested in is `cardio`. Let's have a brief look at the features again:\n",
    "\n",
    "| Feature | Type | Explanation |\n",
    "| :- | :- | :-\n",
    "| age | numeric | The age of the patient in years \n",
    "| sex | binary | Female == 0, Male == 1\n",
    "| height | numeric | Measured weight of the patient (kg)\n",
    "| weight | numeric | Measured weight of the patient (cm)\n",
    "| ap_hi | numeric | Measured Systolic blood pressure\n",
    "| ap_lo | numeric | Measured Diastolic blood pressure\n",
    "| smoke | binary | A subjective feature based on asking the patient whether or not he/she smokes\n",
    "| alco | binary | A subjective feature based on asking the patient whether or not he/she consumes alcohol\n",
    "| active | binary |  A subjective feature based on asking the patient whether or not he/she exercises regularly\n",
    "| cholesterol | categorical | Cholesterol associated risk information evaluated by a doctor\n",
    "| gluc | categorical | Glucose associated risk information evaluated by a doctor\n",
    "\n",
    "But first, we need data for the task. The code for loading the data into the environment is provided for you. The code should work but make sure that you have the CSV file of the data in the same directory where you have this notebook file.\n",
    "\n",
    "**Exercise 1 A)**\n",
    "\n",
    "Take a random sample of 1000 rows from the dataframe using a fixed random seed. Print the first 15 rows to check that everything is ok with the dataframe.\n",
    "\n",
    "*Note: As mentioned, the data remains the same, but cholesterol has been one-hot-encoded for you already. There's a new variable, `gluc` (about glucose aka blood sugar levels), which is also one-hot-encoded for you. It has similar values as `cholesterol`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading code provided\n",
    "# ------------------------------------------------------\n",
    "# The data file should be at the same location than the \n",
    "# exercise file to make sure the following lines work!\n",
    "# Otherwise, fix the path.\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Path for the data\n",
    "data_path = 'ex2_cardio_data.csv'\n",
    "\n",
    "# Read the CSV file \n",
    "cardio_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Resample and print 15 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfq_3_WNoDw3"
   },
   "source": [
    "----\n",
    "\n",
    "We have the data so now, let's put it to use. All the analyses will be done based on this sample of 1000.\n",
    "\n",
    "To teach the k-NN algorithm (or any other machine learning algorithm) to recognize patterns, we need <font color = lightcoral>training data</font>. However, to assess how well a model has learned these patterns, we require <font color = lightcoral>test data</font> which is new and unseen by the trained model. It's important to note that the test set is not revealed to the model until after the training is complete.\n",
    "\n",
    "So, to *estimate the performance of a model*, we may use a basic <font color = lightcoral>train-test split</font>. The term \"split\" is there because we literally split the data into two sets.\n",
    "\n",
    "Before the exercise itself, we might as well discuss about the reproducibility of experiments we conduct in research. It can be quite a nightmare for some if code spewed out only random results. To address this, we can set a **random seed** to ensure that any random processes, such as splitting our dataset into training and test sets, yield consistent results across multiple runs. By using a fixed random seed, we enhance the reproducibility of our experiments, making it easier to validate findings. In fact, we already used one when sampling our subset from the loaded dataset.\n",
    "\n",
    "**Exercise 1 B)**\n",
    "\n",
    "Gather the features into one array and the target variable into another array. Create training and test data by splitting the data into training (80%) and test (20%) sets. Use a fixed random seed to ensure that even if you execute this cell hundreds of times, you will get the same split each time.\n",
    "\n",
    "- Do you need stratification for our dataset? Explain your decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> \\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "**Exercise 1 C)** \n",
    "\n",
    "Standardize the numerical features in both the train and test sets.\n",
    "\n",
    "- Explain how the k-NN model makes predictions about whether or not a patient has cardiovascular disease (CVD) when the features are not standardized. Specifically, discuss how the varying scales of different features can influence the model's predictions, and how standardization would change this influence.\n",
    "\n",
    "\n",
    "*Note: Some good information about preprocessing and how to use it for train and test data can be found https://scikit-learn.org/stable/modules/preprocessing.html*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> \\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muqCazPIoDw4"
   },
   "source": [
    "-------\n",
    "\n",
    "It's time for us to train the model!\n",
    "\n",
    "**Exercise 1 D)**\n",
    "\n",
    "Train a k-NN model with $k=3$. Print out the confusion matrix and use it to compute the accuracy, the precision and the recall.\n",
    "- What does each cell in the confusion matrix represents in the context of our dataset?\n",
    "- How does the model perform with the different classes? Where do you think the differences come from? Interpret the performance metrics you just computed.\n",
    "- With our dataset, why should you be a little more cautious when interpreting the accuracy?\n",
    "\n",
    "*Note: We are very aware that there are functions available for these metrics, but this time, please calculate them using the confusion matrix.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NMR7Y2s6oDw4",
    "outputId": "33bd42f3-a25c-47de-cb12-908d698d08af"
   },
   "outputs": [],
   "source": [
    "### Code - the kNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> \\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQfehqAioDw4"
   },
   "source": [
    "__________\n",
    "## <font color = royalblue> 2. Classification accuracy using leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPZx-6JLoDw5"
   },
   "source": [
    "While the train-test split may provide us with an unbiased estimate of the performance, we only evaluate the model once. Especially when dealing with small datasets, a test set itself will be very small. How can we be sure that the evaluation is accurate with this small test set and not just a good (or bad) luck? And what if we'd like to compare two models and the other seems to be better -- how can we be sure that it's not just a coincidence?\n",
    "\n",
    "Well, there's a great help available and it's called <font color = royalblue>cross-validation</font>. With its help, we can split the dataset into multiple different training and test sets, which allows us to evaluate models across various data partitions. This time, we'll take a closer look at the <font color = royalblue>leave-one-out cross-validation</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Let's keep the focus on detecting the CVD, so once again we utilize the k-NN model (with $k=3$) to predict the precense of the disease. Now, apply leave-one-out cross-validation to assess whether the k-NN model is suitable for addressing the problem. You may use the entire sample of 1000 on this task.\n",
    "\n",
    "- What can you say about the accuracy compared to the previous task?\n",
    "- What do you think: Does the k-NN model work for the problem in hand? Explain your answer.\n",
    "\n",
    "*Tip: This can certainly be done manually, but `cross_val_score` is also a very handy function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "znHjVwKDoDw5",
    "outputId": "3b73cc4a-eef7-4e9e-be17-19d79df71033"
   },
   "outputs": [],
   "source": [
    "### Code - Leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = royalblue> \\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88BjCQL6oDw5"
   },
   "source": [
    "____________\n",
    "## <font color = forestgreen> 3. Model selection with leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8fcES_LoDw5"
   },
   "source": [
    "So far, we've trained one model at a time and I've given the value of k for you. Accuracy is what it is (no spoilers here), but could we still do a little better? Let's explore that possibility through a process known as <font color=green>hyperparameter tuning</font>. The cross-validation is especially important tool for this task. Note here, that model selection and model evaluation (or assessment) are two different things: We use model selection to estimate the performance of various models to identify the model which is most likely to provide the \"best\" predictive performance for the task. And when we have found this most suitable model, we *assess* its perfomance and generalisation power on unseen data.\n",
    "\n",
    "This time, we're going to train multiple models, let's say 30, and our goal is to select the best K-Nearest Neighbors model from this set. Most models come with various hyperparameters that require careful selection, and the k-NN model is no exception. Although we're talking about the number of neighbors here, it's important to note that k-NN also has several other hyperparameters, such as the used distance measure. However, for the sake of simplicity, this time we'll focus solely on fine-tuning the number of nearest neighbors, that is, the value of k, and use default values for all the other hyperparameters. \n",
    "\n",
    "Let's focus on the model selection part here for the sake of comprehending the cross-validation itself. We'll get later on the whole pipeline, which also includes model assessment.\n",
    "\n",
    "**Exercise 3**\n",
    "\n",
    "Find the optimal k value from a set of $k=1...30$ using leave-one-out cross-validation. Plot the accuracies vs. the k values. Again, you may use the entire sample of 1000 on this task.\n",
    "\n",
    "- Which value of k produces the best accuracy when using leave-one-out cross-validation? Compare the result to the previous model with $k=3$.\n",
    "- If the number of k is still increased, what is the limit that the accuracy approaches? Why?\n",
    "- Discuss the impact of choosing a very small or very large number of neighbors on the k-NN model's ability to distinguish between the healthy individuals and the ones with CVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RJ7570_ZoDw6",
    "outputId": "91caaf74-2636-46f7-a4e3-4e5ba98e4c4e"
   },
   "outputs": [],
   "source": [
    "### Code - Select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Plot the accuracies vs. the values for k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________\n",
    "\n",
    "## <font color = darkorange> 4. Ridge regression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous exercises were about classification. Now, we are ready to see another kind of supervised learning - regression - as we are changing our main goal from predicting discrete classes (healthy/sick) to estimating continuous values. The following exercises are going to involve utilizing one regression model, <font color = darkorange>Ridge Regression</font>, and our goal is to evaluate the performance of this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the dataset to make the following exercises more intuitive. The new dataset is about brushtail possums and it includes variables such as\n",
    "\n",
    "\n",
    "| Feature | Type | Explanation |\n",
    "| :- | :- | :-\n",
    "|sex|binary| Sex, either male (0) or female (1)\n",
    "|age|numeric| Age in years\n",
    "|len_head|numeric| Head length in mm\n",
    "|width_skull|numeric| Skull width in mm\n",
    "|len_earconch|numeric| Ear conch length in mm\n",
    "|width_eye|numeric| Distance from medial canthus to lateral canthusof right eye, i.e., eye width in mm\n",
    "|len_foot|numeric| Foot length in mm\n",
    "|len_tail|numeric| Tail length in mm\n",
    "|chest|numeric| Chest grit in mm\n",
    "|belly |numeric| Belly grit in mm\n",
    "|len_total|numeric| Total length in mm\n",
    "\n",
    "In this case, our target variable will be *the age of the possum*. The data for this exercise has been modified from the original source.\n",
    "\n",
    "There's the code chunk for loading data provided again. <font color = red>Again, the data file should be located in the same directory as this notebook file!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading code provided\n",
    "# ------------------------------------------------------\n",
    "# The data file should be at the same location than the \n",
    "# exercise file to make sure the following lines work!\n",
    "# Otherwise, fix the path.\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Data path\n",
    "data_path = 'ex2_possum_data.csv'\n",
    "\n",
    "# Load the data \n",
    "possum_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "Regression allows us to examine <font color = darkorange>relationships between two or more variables</font>. This relationship is represented by an *equation*, which itself represents how a change in one variable affects another on average. For example, we could examine how a change in possum's total length affects, on average, its estimated age.\n",
    "\n",
    "We start by examining those relationships between the variables in the given dataset.\n",
    "\n",
    "\n",
    "**Exercise 4 A)**\n",
    "\n",
    "Plot pairwise relationships between the age variable and the others where you color the samples based on the sex variable. \n",
    "\n",
    "- Which body dimensions seem to be most correlated with age? And are there any variables that seem to have no correlation with it?\n",
    "- Are there any differences in the correlations between males and females?\n",
    "\n",
    "*Tip: `seaborn.pairplot()` is handy with the parameters `(x,y)_vars` and `hue`. You actually can fit a linear model to draw a regression line with the parameter `kind` set to `\"reg\"`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = darkorange>\\<Write your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "Before the regression analysis itself, let's check that our dataset is in a proper format. We'll also perform the train-test split as we're going to test the overall performance of the model using the test set.\n",
    "\n",
    "**Exercise 4 B)**\n",
    "\n",
    "Do you need to prepare the data a little? Explain your decision. Perform the train-test (80/20) split. \n",
    "\n",
    "*Note: Set the features in the dataframe named as `possum_X` so you can play around with the upcoming code snippet.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = darkorange>\\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "Regarding Ridge Regression, we'll focus on the hyperparameter called $\\lambda$ (read as 'lambda'), the regularization term (or penalty term or L2 penalty, how ever we'd like to call it).\n",
    "\n",
    "**Exercise 4 C)**\n",
    "\n",
    "Fit a ridge regression model with the whole training set. For the hyperparameter 'lambda', use 64. Evaluate the model using the test set and describe the results. For evaluating on the test set, use a metric called mean absolute error (MAE).\n",
    "\n",
    "- How well did the model perform in estimating the possums' ages?\n",
    "- How do you interpret the MAE in our case when the target variable is age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code - Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = darkorange>\\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fitted the regression model, let's break it down for better understanding what is actually happening here. Remember that the model here is essentially just a linear regression model with an added regularization term to deal with e.g overfitting and multicollinearity. We can write the equation used by the model to predict an opossum's age as:\n",
    "\n",
    "$$\n",
    "\\text{Predicted age} = w_1 \\times \\text{Sex} + w_2 \\times \\text{Head length} + w_3 \\times \\text{Skull width} + ... + w_{10} \\times \\text{Total length} + \\text{Bias}\n",
    "$$\n",
    "\n",
    "As mentioned earlier, regression focuses on the relationships between the features and the target variable. In the equation above, each feature contibutes a certain amount to the predicted age, based on the weight $w_i$ learned for that feature. For example, if the total length of an opossum has a large positive weight, it suggests that opossums with greater length are predicted to be older. On the other hand, if the skull width of an opossum has a negative weight, it indicates that opossums with wider skulls are predicted to be younger. In this case, as skull width increases, the predicted age decreases.\n",
    "\n",
    "Different classes have different class attributes that you can access after e.g. fitting a model, and the `Ridge` class is no exception: For example, the `coef_` variable contains the learned weights $w_1, ..., w_{10}$ that represent the relationship between the features and the target (a.k.a age) variable. The `intercept_` variable holds the bias term (or the intercept, however we wanna call it). \n",
    "\n",
    "We can now write down the equation used by our fitted model. You can experiment with it by adjusting the regularization term or using a different sample, if you'd like, to see how the weights and bias change. This is just extra!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: To make this code chunk to work with the already fitted model, \n",
    "#       the model variable needs to be named as `ridge_model`. Also, the\n",
    "#       initial feature dataframe is named here as `possum_X`.\n",
    "\n",
    "\n",
    "coefficients = ridge_model.coef_ # CHANGE THE VARIABLE NAME IF NOT WROTE AS THIS\n",
    "bias = ridge_model.intercept_ # # CHANGE THE VARIABLE NAME IF NOT WROTE AS THIS\n",
    "feature_names = possum_X.columns # CHANGE THE VARIABLE NAME HERE IF NOT AS WROTE AS THIS\n",
    "\n",
    "# Let's write the equation\n",
    "equation = 'Predicted age = '\n",
    "for i in range(len(coefficients)):\n",
    "    equation += f'{coefficients[i]:.3f}*{feature_names[i]} + '\n",
    "\n",
    "equation += f'{bias:.3f}'\n",
    "print(equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "## <font color = slategrey> BONUS: Feature selection - most useful features in predicting cardiovascular diseases </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop here and get the \"pass\" grade! To get the pass with honors, you need to do the following exercise. This means you'll get one bonus point for the exam.\n",
    "\n",
    "The exercise may require you to do some research of your own. You are also required to **explain** the steps you choose with your own words, and show that you tried to understand the idea behind the task. There's no single correct solution for this so just explain what you did and especially ***why*** you did it. Please note that submitting only code will not be awarded a pass with honors.\n",
    "\n",
    "----------------\n",
    "\n",
    "\n",
    "Due to the lack of resources and time, doctors can't measure all the values represented in the given cardio dataset. Fortunately, eager students are willing to help: Your task is to identify <font color = slategrey>five [5] most useful features</font> for predicting the presence of the CVD from the dataset. The steps needed for this job are presented above except the feature selection part. You must remember not to leak any information from the test set when selecting the features, i.e., you try to find those five features using only the training set.\n",
    "\n",
    "Regarding the feature selection itself, you're asked to use <font color = slategrey>Random Forest</font>. To do this, use the Random Forest classifier's built-in feature importance estimation in scikit-learn. Explain briefly the working of the model on the given cardio dataset: How does the model select features that are relevant in predicting CVD?\n",
    "\n",
    "Evaluate the model of your choice using accuracy and the area under the ROC curve (AUC). Draw the corresponding curve in a plot.  **Discuss** your findings and results.\n",
    "\n",
    "What goes wrong in your AUC analysis, if you use the predictions from the `predict()` function instead of the `predict_proba()` function to calculate the AUC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code - Bonus task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = slategrey>\\<Write your answer here\\></font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DADK2020_ex3_Valtteri_Nieminen.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "180px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
